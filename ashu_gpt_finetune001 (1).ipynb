{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7lmwFaRqoQH",
        "outputId": "57a06d05-57d4-48f6-fe32-a908d5c94891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: gradio==3.50 in /usr/local/lib/python3.10/dist-packages (3.50.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.112.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.23.5)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (3.10.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (0.30.5)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio==3.50) (2024.6.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.50) (3.15.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.50) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.50) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.50) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.50) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.50) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.50) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.50) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.50) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.50) (2024.7.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.50) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.50) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.50) (0.37.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50) (1.3.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.50) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install numpy\n",
        "!pip install tiktoken\n",
        "!pip install gradio==3.50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import tiktoken\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "tegjvKgTrcIZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my api key\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "zV_omMhIrkM3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # preparing dataset in the format of GPT\n",
        "# {\n",
        "#     \"messages\": [\n",
        "#         { \"role\": \"system\" , \"content\": \"ashutoshh helper bot with smart communication\" },\n",
        "#         { \"role\": \"user\" , \"content\": \"hey can you explain heap memory in java \"},\n",
        "#         { \"role\": \"assistant\", \"content\": \"response of above question\"}\n",
        "#     ],\n",
        "#     \"messages\": [\n",
        "#         { \"role\": \"system\" , \"content\": \"ashutoshh helper bot with smart communication\" },\n",
        "#         { \"role\": \"user\" , \"content\": \"hey can you explain heap memory setting in  java \"},\n",
        "#         { \"role\": \"assistant\", \"content\": \"response of above question to setup java heal memory\"}\n",
        "#     ]\n",
        "# }\n",
        "#Load CSV data in\n",
        "csv_file_path = '/content/sample_data/hotel_hospitality_data.csv'\n",
        "cleaned_data = []\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8-sig') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "    for row in csv_reader:\n",
        "        for cell in row:\n",
        "            try:\n",
        "                # Replace square brackets and inner double quotes that are problematic\n",
        "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
        "\n",
        "                # Load each cell as a JSON object\n",
        "                cell_json = json.loads(cell)\n",
        "\n",
        "                # Now that the content is clean, append to cleaned_data list\n",
        "                cleaned_data.append(cell_json)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
        "\n",
        "jsonl_file_path = '/content/sample_data/hotel_hospitality_data_formated.jsonl'\n",
        "# Write cleaned data to a JSONL file\n",
        "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
        "    for item in cleaned_data:\n",
        "        jsonl_file.write(json.dumps(item) + '\\n')"
      ],
      "metadata": {
        "id": "U75RfixLrn1a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we specify the data path and open the JSONL file\n",
        "\n",
        "data_path = '/content/sample_data/hotel_hospitality_data_formated.jsonl'\n",
        "\n",
        "# Load dataset\n",
        "with open(data_path) as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# We can inspect the data quickly by checking the number of examples and the first item\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")\n",
        "\n",
        "# Token counting functions\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oprleCLUXIBx",
        "outputId": "be721d50-7423-450a-ec81-6e6799b57bc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 18\n",
            "First example:\n",
            "{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}\n",
            "{'role': 'user', 'content': 'Where is the gym located?'}\n",
            "{'role': 'assistant', 'content': \"Our gym is located on the 2nd floor. I'll be happy to show you the way if you need help!\"}\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 68, 91\n",
            "mean / median: 76.38888888888889, 74.0\n",
            "p5 / p95: 71.4, 87.5\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 16, 39\n",
            "mean / median: 24.444444444444443, 23.0\n",
            "p5 / p95: 19.0, 35.5\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "# Calculate the estimated cost for fine-tuning\n",
        "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
        "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
        "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6E7JuCcXf_f",
        "outputId": "e376597b-be54-4364-8450-c86f0a369ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has ~1375 tokens that will be charged for during training\n",
            "By default, you'll train for 5 epochs on this dataset\n",
            "By default, you'll be charged for ~6875 tokens\n",
            "Estimated cost for fine-tuning: approximately $0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the dataset as a JSONL file\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for conversation in conversations:\n",
        "            json_line = json.dumps(conversation)\n",
        "            file.write(json_line + '\\n')\n",
        "\n",
        "# Specify the path where you want to save the JSONL file in your Google Drive\n",
        "jsonl_file_path = '/content/sample_data/helper-example-json-clean.jsonl'\n",
        "# Save the dataset to the specified file path\n",
        "save_to_jsonl(dataset, jsonl_file_path)"
      ],
      "metadata": {
        "id": "M-xjvMVfYOHB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload data for training\n",
        "training_file_name = '/content/sample_data/helper-example-json-clean.jsonl'\n",
        "\n",
        "training_response = openai.File.create(\n",
        "    file=open(training_file_name, \"rb\"), purpose=\"fine-tune\"\n",
        ")\n",
        "training_file_id = training_response[\"id\"]\n",
        "\n",
        "#Gives training file id\n",
        "print(\"Training file id:\", training_file_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBEcKB4JYrsa",
        "outputId": "bbc1356b-6f2a-4a42-d518-bb1ba7473666"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file id: file-f6MZKsulItHIjd51eLRqd70C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Fine-Tuning Job\n",
        "suffix_name = \"chatner-bot\"\n",
        "\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=training_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    suffix=suffix_name,\n",
        "   # n_epochs = 10\n",
        ")\n",
        "\n",
        "job_id = response[\"id\"]\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAB5GWSpZ0dI",
        "outputId": "17ee9dae-f49e-49e6-c64e-8614f3a09139"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-P7b6WTON4JGaSe5dUu4BzwgE\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1722909952,\n",
            "  \"finished_at\": null,\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"organization_id\": \"org-sW9LYXODuBOJUwL7Lo4RNLZu\",\n",
            "  \"result_files\": [],\n",
            "  \"status\": \"validating_files\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-f6MZKsulItHIjd51eLRqd70C\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": \"auto\",\n",
            "    \"batch_size\": \"auto\",\n",
            "    \"learning_rate_multiplier\": \"auto\"\n",
            "  },\n",
            "  \"trained_tokens\": null,\n",
            "  \"error\": {},\n",
            "  \"user_provided_suffix\": \"chatner-bot\",\n",
            "  \"seed\": 1212571061,\n",
            "  \"estimated_finish\": null,\n",
            "  \"integrations\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list events as fine-tuning progresses\n",
        "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBFRgTAfb3_P",
        "outputId": "adb320f8-b9f8-44e1-d697-f2f14d15095b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 45/90: training loss=0.43\n",
            "Step 46/90: training loss=1.14\n",
            "Step 47/90: training loss=0.18\n",
            "Step 48/90: training loss=0.20\n",
            "Step 49/90: training loss=0.39\n",
            "Step 50/90: training loss=0.32\n",
            "Step 51/90: training loss=0.64\n",
            "Step 52/90: training loss=0.34\n",
            "Step 53/90: training loss=0.19\n",
            "Step 54/90: training loss=0.50\n",
            "Step 55/90: training loss=0.31\n",
            "Step 56/90: training loss=0.07\n",
            "Step 57/90: training loss=0.25\n",
            "Step 58/90: training loss=0.20\n",
            "Step 59/90: training loss=0.13\n",
            "Step 60/90: training loss=0.10\n",
            "Step 61/90: training loss=0.05\n",
            "Step 62/90: training loss=0.09\n",
            "Step 63/90: training loss=0.09\n",
            "Step 64/90: training loss=0.52\n",
            "Step 65/90: training loss=0.29\n",
            "Step 66/90: training loss=0.33\n",
            "Step 67/90: training loss=0.51\n",
            "Step 68/90: training loss=0.38\n",
            "Step 69/90: training loss=0.86\n",
            "Step 70/90: training loss=0.21\n",
            "Step 71/90: training loss=0.24\n",
            "Step 72/90: training loss=0.59\n",
            "Step 73/90: training loss=0.16\n",
            "Step 74/90: training loss=0.44\n",
            "Step 75/90: training loss=0.05\n",
            "Step 76/90: training loss=0.43\n",
            "Step 77/90: training loss=0.03\n",
            "Step 78/90: training loss=0.06\n",
            "Step 79/90: training loss=0.75\n",
            "Step 80/90: training loss=0.05\n",
            "Step 81/90: training loss=0.13\n",
            "Step 82/90: training loss=0.53\n",
            "Step 83/90: training loss=0.04\n",
            "Step 84/90: training loss=0.15\n",
            "Step 85/90: training loss=0.29\n",
            "Step 86/90: training loss=0.20\n",
            "Step 87/90: training loss=0.18\n",
            "Step 88/90: training loss=0.06\n",
            "Step 89/90: training loss=0.08\n",
            "Step 90/90: training loss=0.26\n",
            "Checkpoint created at step 54 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDPIC:ckpt-step-54\n",
            "Checkpoint created at step 72 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDQOW:ckpt-step-72\n",
            "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDM1s\n",
            "The job has successfully completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve fine-tune model id\n",
        "response = openai.FineTuningJob.retrieve(job_id)\n",
        "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
        "\n",
        "print(response)\n",
        "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFtCVRgyb9rP",
        "outputId": "64dc80f7-a5b6-48cd-b02e-0aa7405b38fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"fine_tuning.job\",\n",
            "  \"id\": \"ftjob-P7b6WTON4JGaSe5dUu4BzwgE\",\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"created_at\": 1722909952,\n",
            "  \"finished_at\": 1722910266,\n",
            "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDM1s\",\n",
            "  \"organization_id\": \"org-sW9LYXODuBOJUwL7Lo4RNLZu\",\n",
            "  \"result_files\": [\n",
            "    \"file-a4orKBauNpudoFVar5HnUbhb\"\n",
            "  ],\n",
            "  \"status\": \"succeeded\",\n",
            "  \"validation_file\": null,\n",
            "  \"training_file\": \"file-f6MZKsulItHIjd51eLRqd70C\",\n",
            "  \"hyperparameters\": {\n",
            "    \"n_epochs\": 5,\n",
            "    \"batch_size\": 1,\n",
            "    \"learning_rate_multiplier\": 2\n",
            "  },\n",
            "  \"trained_tokens\": 6695,\n",
            "  \"error\": {},\n",
            "  \"user_provided_suffix\": \"chatner-bot\",\n",
            "  \"seed\": 1212571061,\n",
            "  \"estimated_finish\": null,\n",
            "  \"integrations\": []\n",
            "}\n",
            "\n",
            "Fine-tuned model id: ft:gpt-3.5-turbo-0125:personal:chatner-bot:9t3xDM1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it out!\n",
        "test_messages = []\n",
        "\n",
        "system_message = \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"\n",
        "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "user_message = \"Where should we park\"\n",
        "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "print(test_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKAxN2u5d1Y0",
        "outputId": "332b86f8-1092-4031-ada7-8fa143fa08e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': \"You are an overly friendly hospitality chatbot named Chatner who just loves to help people, and you're not satisfied unless the customer is completely satisfied.\"}, {'role': 'user', 'content': 'Where should we park'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI Chat Completions to get the response\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
        "    messages=test_messages,\n",
        "    temperature=0,\n",
        "    max_tokens=500\n",
        ")\n",
        "print(response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udUjqlQFd2YI",
        "outputId": "6b543186-3d53-4f34-86f3-7e1ea6f212ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our parking lot is located right next to the hotel. You can park there for free. I'll be happy to provide directions if needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradio for a better UI\n",
        "def generate_completion(user_prompt):\n",
        "    hidden_context = \"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": hidden_context},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=fine_tuned_model_id,\n",
        "        messages=messages,\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "iface = gr.Interface(fn=generate_completion,\n",
        "                     inputs=gr.inputs.Textbox(lines=5, placeholder='Question about the Airbnb?'),\n",
        "                     outputs='text',\n",
        "                     title=\"Chatner The Airbnb Helper\",\n",
        "                     input_labels=\"Question\",\n",
        "                     output_labels=\"Response\")\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "fOHfa7BIfa59",
        "outputId": "92d93909-a9ca-4fbb-8f90-1fa6fd4c350b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-50344e08f256>:17: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs=gr.inputs.Textbox(lines=5, placeholder='Question about the Airbnb?'),\n",
            "<ipython-input-3-50344e08f256>:17: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(lines=5, placeholder='Question about the Airbnb?'),\n",
            "<ipython-input-3-50344e08f256>:17: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(lines=5, placeholder='Question about the Airbnb?'),\n",
            "<ipython-input-3-50344e08f256>:16: GradioUnusedKwargWarning: You have unused kwarg parameters in Interface, please remove them: {'input_labels': 'Question', 'output_labels': 'Response'}\n",
            "  iface = gr.Interface(fn=generate_completion,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "IMPORTANT: You are using gradio version 3.50.0, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://d1a9a205dcdf3399d3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d1a9a205dcdf3399d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mn6_FjJ5hArZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}